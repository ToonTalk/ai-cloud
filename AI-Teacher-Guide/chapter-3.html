<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Camera Input - AI Teacher's Guide</title>
<link href="ai-teacher-guide.css" rel="stylesheet">
<link rel="icon" type="image/png" href="images/eCraft2Learn-Favicon.png" />
</head>
<body>
<h2>A teacher's guide to helping students build AI apps and artefacts</h2>
<h3>Chapter 3 - Camera Input</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3>Browser compatibility</h3>
<p>This guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.</p>
<h3>Introduction</h3>
<p>A camera connected to a computer can report the colour of each pixel in an image and not much else.
A description of what is in front of the camera is returned when those pixels are sent to an image recogntion service.
There are many kinds of things an image description may contain.
With speech recogntion the description is what was spoken, how confident the system is, and possible alternatives.
With image recogntion there are many possible descriptions: descriptive tags, possible captions, dominant colours,
location of faces and parts of the face if there are any, and the presence of landmarks, celebrities, well-known entities, and logos.
Text can also be recognised.</p>

<p>A challenge in providing student-friendly programming blocks for image recogntion is that
different AI cloud services report different descriptions in different ways.
We currently provide interfaces to image recogntion services provided by Google, Microsoft, and IBM.
A further challenge is how to provide simple interfaces for simple tasks while still supporting more sophisticated uses and projects.</p>

<p>In the last few years there has been tremendous progress in computer vision.
High performance systems for identifying objects, recognising faces, interpreting sketches, and using medical images to aid diagnosis.
Driverless cars rely heavily upon computer vision.</p>

<h4>API keys are needed</h4>
<p>Chrome has builtin support for speech input and output.
No browser, however, currently supports image recogntion.
To access vision recogntion services from companies such as Google, Microsoft, or IBM
one needs to <a href="https://github.com/ToonTalk/ai-cloud/wiki" target="_blank">open an account</a>.
Accounts are free and provide some degree of free usage.
IBM premits 250 free vision queries per day, Microsoft 5000 per month, and Google 1000 per month.
To try the vision blocks described here you need at least one account.
Comparing and constrasting the results from different services is an interesting way to gain some insight into how these services work.</p>

<p>In projects there is a different way of providing keys to Snap! but in this chapter you can paste your keys below
and they'll be passed to the services as you use example blocks.
<br><label>Copy and paste your Google key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Google image key" type="text"></label>
<br><label>Copy and paste your Microsoft key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Microsoft image key" type="text"></label>
<br><label>Copy and paste your IBM Watson key here: <input id="IBM Watson image key" type="text"></label></p>

<h4>A simple image recogntion block</h4>
<p>This block takes a picture, sends it to the AI vision cloud server provider, waits for a response, and then
reports a list of labels of the photo.
The list is ordered by how confident the vision provider is that the label matches the image.</p>

<figure>
<div class="iframe-container"
     style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/image labels reporter.xml"> 
</iframe></div>
<figcaption>A block for reporting the labels describing what the camera is showing. TRY IT</figcaption>
</figure>

<h4>Displaying the image that was sent for recogntion</h4>
<p>The 'show current photo' block will display the most recent image sent to the specified AI vision services
as the background of the Snap! stage.
To try it first obtain some labels from a vision service.
The 'use camera to create costume' block takes a new picture and adds it as a costume to the current sprite.</p>

<figure>
<div class="iframe-container" style="width: 1000px; height: 400px;">
<iframe class="iframe-clipped"
        style="margin-left: -500px; margin-top: 200px;"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/image labels reporter and show photo.xml"> 
</iframe></div>
<figcaption>After sending a photo for analysis you can use a block to display the image. TRY IT</figcaption>
</figure>

<h4>A sample program using image recogntion</h4>
<p>Here is a program that can contact any of Google, Microsoft, or IBM and displays the tags returned.</p>

<figure>
<iframe style="width: 800px; height: 600px"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera and say.xml"
        run_full_screen="true"> 
</iframe></div>
<figcaption>Click on the AI cloud provider logos and see what happens</figcaption>
</figure>

<h4>A sample program combing image and speech recogntion and speech output</h4>
<p>This program is similar to the previous one except it is listening for the words "Google", "Microsoft", or "Watson".
When it hears one of these words it contacts the service.
When the response is received it turns it into speech.
As a demo it is impressive if you say things like "Tell me Google what do you see?" and "What do you see Microsoft?".
It is a nice illustration of appearing more intelligent than it is.
Though a discussion with the students on how and where this program is intelligent and where it isn't could be good.
</p>

<figure>
<iframe style="width: 800px; height: 600px"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera, listen, and speak.xml"
        full_screen="true"> 
</iframe></div>
<figcaption>Click the green flag and then say a sentence containing "Google", "Microsoft", or "Watson"</figcaption>
</figure>

<h4>Advanced image recogntion blocks</h4>
<p>Image recoognisers can do more than label images.
Some can detect and locate faces.
Some of those will estimate the age of the person and their gender.
Some recognise landmarks and logos.
Many can recognise characters in text.
A problem creating Snap! blocks that provide this functionality is that
different services have different capabilities and different structures to capture their responses.
Compare the documentation for the vision services from
<a href="https://cloud.google.com/vision/" target="_blank">Google</a>,
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>,
and
<a href="https://www.ibm.com/watson/services/visual-recognition/" target="_blank">IBM Watson</a>.</p>

<p>The 'Recognize new photo' block addresses this problem by reporting a data structure capturing the entire response from
the vision service.
It is a Snap! list that contains lists that contains more data that might be text for labels,
numbers for confidence scores, and even more lists for complex data.
You can double click on the <img src="images/sub-list-icon.png" alt="icon for opening lists in lists"> icons to drill down the structure.
The 'Current image property' block takes an argument that describes what piece of the response structure should be reported.
For example, Microsoft offers possible captions that can be found by following the path "description captions text".
Common useful blocks are defined that use the 'Current image property' block internally
to get the labels and the confidence scores of the labels for each of the supported vision service providers.
Note that after a response is received from any of the AI services it is stored so that calls to 'Current image property'
use the most recent response rather than ask for a new one.
This is because one's project may rely on multiple pieces of the response.
</p>

<figure>
<div class="iframe-container" style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/recognize new photo.xml"> 
</iframe></div>
<figcaption>A block for reporting the entire analysis response. TRY IT</figcaption>
</figure>

<p>The 'Recognize new photo' reporter block is implemented using the 'Ask <provider> to say what it sees' block.
This block does not wait for a response, instead it runs the user's blocks when a response is received.
It isn't as convenient 'Recognize new photo' but it can support more complex usage.
</p>

<figure>
<div class="iframe-container" style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/recognize asynchronous.xml"> 
</iframe></div>
<figcaption>A block for obtaining the entire analysis response as a callback. TRY IT</figcaption>
</figure>

<h4>How to provide API keys</h4>
<p>This chapter uses the API keys provided in the text areas above.
When constructing a project there are two ways to provide keys:
<ol>
<li>Add extra information to the page's URL.
Appending "&IBM Watson image key=...&Google image key=...&Microsoft image key=..." to the project URL will
provide all three keys (after "..." is replaced by the real keys). 
If some service providers aren't being used then there is no need to include their keys in the URL.</li>
<li>Edit any of the following special reporters:</li>
</ol>

<figure>
<div class="iframe-container" style="width: 800px; height: 280px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/blank keys.xml"> 
</iframe></div>
<figcaption>Reporters for API keys that users edit to provide service keys. TRY IT</figcaption>
</figure>

<p>Because all the AI cloud services are commericial services heavy use can be costly.
Consequently it is best if each student has their own account and minimises the sharing of their keys.
This conflicts with one of the great things about tools like Scratch and Snap! --
that it is so easy to share one's projects with a wider community.
Adding the keys to the URL solves this problem if one is careful to keep the URL with keys private
and share the version with the keys.
It is more awkward to maintain private and public versions when using reporters for keys.</p>

<h4>What about video recogntion services?</h4>
<p>Some AI cloud providers such as <a href="https://cloud.google.com/video-intelligence/" target="_blank">Google</a>
can accept a video stream and produces labels.
It can also detect scene changes.
This service can be expensive, though Google analyses 1000 minutes per month at no cost.
There currently are no blocks for supporting video input.</p>

<h3>Additional resources</h3>
<a href="https://cloud.google.com/vision/" target="_blank">Google</a>,
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>,
and
<a href="https://www.ibm.com/watson/services/visual-recognition/" target="_blank">IBM Watson</a>
https://www.upwork.com/hiring/data/comparing-image-recognition-apis/


<h3>Where to get these blocks to use in projects</h3>
<p>All of these speech output blocks as well other AI blocks are available to use in <a href="https://snap.berkeley.edu/" target="_blank">Snap!</a> in
this <a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=toontalk&ProjectName=eCraft2Learn" target="_blank">Snap project</a>.
They can also be used together with blocks for controlling Arduinos by downloading and then importing
<a href="https://toontalk.github.io/ai-cloud/AI-Teacher-Guide/eCraft2Learn%20S4A.xml" download>this file</a>
into <a href="http://snap4arduino.rocks/" target="_blank">Snap4Arduino</a>.

<h3>Learn about machine learning</h3>
<p>The next chapter is planned to be about machine learning.</p>

<script src="acknowledgements.js"></script>
</body></html>